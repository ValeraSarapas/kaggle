{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Dogs vs Cats Investigation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1070 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following lines in order to set up the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We set the \"seed\" so we make the results a bit more predictable.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type 'sample/' if you want to work on a smaller dataset.\n",
    "path = ''\n",
    "# Depending on your GPU you should change this. For a GTX 970 this is a good value. \n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the timestamp that we are going to use when saving files.\n",
    "timestamp = '175814012017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some useful paths to save files (e.g weights)\n",
    "files_path = path + 'files/'\n",
    "models_path = path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batches(path, shuffle=[False, False, True], augmentation=False):\n",
    "    \"\"\"\n",
    "    Load different batches that we'll use in our calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    gen = image.ImageDataGenerator()\n",
    "    val_batches = gen.flow_from_directory(path + 'valid', target_size=(224,224),\n",
    "                    class_mode='categorical', shuffle=shuffle[0], batch_size=batch_size)\n",
    "    test_batches = gen.flow_from_directory(path + 'test', target_size=(224,224),\n",
    "                    class_mode='categorical', shuffle=shuffle[1], batch_size=batch_size)\n",
    "    \n",
    "    # We only want Data augmentation for the training set.\n",
    "    if augmentation:\n",
    "        gen = image.ImageDataGenerator(rotation_range=20, width_shift_range=0.1, shear_range=0.05,\n",
    "                                       height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "    train_batches = gen.flow_from_directory(path + 'train', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=shuffle[2], batch_size=batch_size)\n",
    "\n",
    "    return train_batches, val_batches, test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finetune(model):\n",
    "    \"\"\"\n",
    "    Removes the last layer (usually Dense) and replace it by another one more fitting.\n",
    "    This is useful when using a pre-trained model like VGG.\n",
    "    \"\"\"\n",
    "    model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.add(Dense(train_batches.nb_class, activation='softmax'))\n",
    "    model.compile(optimizer=RMSprop(lr=0.01, rho=0.7),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagation(model):\n",
    "    \"\"\"\n",
    "    Now we do Backpropagation. Backpropagation is when we want to train not only the last\n",
    "    Dense layer, but also some previous ones. Note that we don't train Convolutional layers.\n",
    "    \"\"\"\n",
    "    layers = model.layers\n",
    "    for layer in layers: layer.trainable=False\n",
    "    # Get the index of the first dense layer...\n",
    "    first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "    # ...and set this and all subsequent layers to trainable\n",
    "    for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(model, path, name, timestamp):\n",
    "    print 'Saving weights: {}.h5'.format(path + name + '_' + timestamp)\n",
    "    model.save_weights(path + '{}_{}.h5'.format(name, timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model, filepath):\n",
    "    print 'Loading weights: {}'.format(filepath)\n",
    "    model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_batches, val_batches, rules, name, timestamp):\n",
    "    \"\"\"\n",
    "    Rules will be something like:\n",
    "        (\n",
    "            (0.01, 3),\n",
    "            (0.1, 2),\n",
    "            ...\n",
    "        )\n",
    "    \"\"\"\n",
    "    for lr, epochs in rules:\n",
    "        model.compile(optimizer=RMSprop(lr=lr, rho=0.7),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print 'Lr: {}, Epoch: {}'.format(lr, i + 1)\n",
    "            model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, verbose=2,\n",
    "                               nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "            \n",
    "            #sys.stdout = open('keras_output.txt', 'w')\n",
    "            #history = model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, verbose=2,\n",
    "            #                              nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "            #sys.stdout = sys.__stdout__\n",
    "            #with open('keras_output.txt') as f:\n",
    "            #    content = f.readlines()\n",
    "            save_weights(model, files_path, '{}_lr{}_epoch{}'.format(\n",
    "                    name, lr, i+1), timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_conv_fc(model):\n",
    "    \"\"\"\n",
    "    Split Convolutional and Dense Layers.\n",
    "    \"\"\"\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_layers, fc_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the weights from the pre-trained model.\n",
    "# NB: Since we're removing dropout, we want to half the weights\n",
    "def proc_wgts(layer): return [o/2 for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model(conv_layers, fc_layers):\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.00001, rho=0.7), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative way to generate Submission file (it has a better score!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights: files/data_augmentation_plus_dropout0_vgg16_data_augentation_to_zero_dropout_lr1e-05_epoch1_102714012017.h5\n"
     ]
    }
   ],
   "source": [
    "load_weights(conv_model, 'files/data_augmentation_plus_dropout0_vgg16_data_augentation_to_zero_dropout_lr1e-05_epoch1_102714012017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_csv(submission_file_name, data, columns):\n",
    "    \"\"\"\n",
    "    Write data according to the Kaggle submission format.\n",
    "    \"\"\"\n",
    "    with open(submission_file_name, 'wb') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(columns)\n",
    "        for key in data.keys():\n",
    "            w.writerow([key, data[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "test_batches = gen.flow_from_directory(path + 'test', target_size=(224,224),\n",
    "                                       class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]\n",
    "#conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "d = {}\n",
    "submission_file_name = 'submission_{}_5_new.csv'.format(timestamp)\n",
    "for idx, filename in enumerate(test_batches.filenames):\n",
    "    # We only want the ID, so remove the folder name and file extension.\n",
    "    result = int(filename[8:-4])\n",
    "    # We use a trick to never show 0 or 1, but 0.05 and 0.95.\n",
    "    # This is required becase log loss penalizes predictions that are confident and wrong.\n",
    "    d[result] = predictions[idx][1].clip(min=0.05, max=0.95)\n",
    "write_submission_csv(submission_file_name, d, ['id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(submission_file_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
