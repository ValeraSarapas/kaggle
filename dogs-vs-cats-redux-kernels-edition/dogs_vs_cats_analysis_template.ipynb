{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Dogs vs Cats Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import zipfile\n",
    "import glob\n",
    "import numpy as np\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Allow relative imports to directories above this directory\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('train.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('test.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "current_dir = os.getcwd()\n",
    "DATA_HOME_DIR = current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create directories\n",
    "os.mkdir('valid')\n",
    "os.mkdir('files')\n",
    "os.mkdir('models')\n",
    "os.mkdir('sample')\n",
    "os.mkdir('sample/train')\n",
    "os.mkdir('sample/valid')\n",
    "os.mkdir('sample/files')\n",
    "os.mkdir('sample/models')\n",
    "os.mkdir('sample/test')\n",
    "os.mkdir('sample/test/unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We move a certain number of files from the train to the valid directory.\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000): os.rename(shuf[i], DATA_HOME_DIR+'/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We copy a certain number of files from the train to the sample/train directory.\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], DATA_HOME_DIR+'/sample/train/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We copy a certain number of files from the valid to the sample/valid directory.\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(50): copyfile(shuf[i], DATA_HOME_DIR+'/sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We copy a certain number of files from the test to the sample/test directory.\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], DATA_HOME_DIR+'/sample/test/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Divide cat/dog images into separate directories\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/train\n",
    "os.mkdir('cats')\n",
    "os.mkdir('dogs')\n",
    "os.rename(\"cat.*.jpg\", \"cats/\")\n",
    "os.rename(\"dog.*.jpg\", \"dogs/\")\n",
    "\n",
    "%cd $DATA_HOME_DIR/sample/valid\n",
    "os.mkdir('cats')\n",
    "os.mkdir('dogs')\n",
    "os.rename(\"cat.*.jpg\", \"cats/\")\n",
    "os.rename(\"dog.*.jpg\", \"dogs/\")\n",
    "\n",
    "%cd $DATA_HOME_DIR/valid\n",
    "os.mkdir('cats')\n",
    "os.mkdir('dogs')\n",
    "os.rename(\"cat.*.jpg\", \"cats/\")\n",
    "os.rename(\"dog.*.jpg\", \"dogs/\")\n",
    "\n",
    "%cd $DATA_HOME_DIR/train\n",
    "os.mkdir('cats')\n",
    "os.mkdir('dogs')\n",
    "os.rename(\"cat.*.jpg\", \"cats/\")\n",
    "os.rename(\"dog.*.jpg\", \"dogs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "os.rename(\"*.jpg\", \"unknown/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/sample/test\n",
    "os.rename(\"*.jpg\", \"unknown/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to keep in mind (Troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Choose always `verbosity=2` when training. Otherwise the notebook will crash.\n",
    "1. Monitor the RAM while running the cells. You might find out that it gets filled quite fast. If this is the case, please wait until it's freed up.\n",
    "2. In theory by running `import gc; gc.collect()` the Garbage collector is called. In practice it doesn't make much difference.\n",
    "3. When it says \"Memory Error\" it means that you have filled the Computer RAM. Try restarting Jupyter.\n",
    "4. When it says \"It cannot allocate...\" it means that you have filled the GPU VRAM. Try restarting Jupyter.\n",
    "5. Don't go with a `batch_size` bigger than `4` when you have 8 GB RAM.\n",
    "6. If you set `shuffle=True` in `gen.flow_from_directory` while getting the training batch, you might get weird results in the \"Removing Dropout\" section.\n",
    "7. If you disable all optimizations in Theano in order to get memory, you might have some exceptions like: `Cuda error 'unspecified launch failure'`\n",
    "8. If you mix up optimizers like Adam or RMSprop, you might have weird results. Always use the same one.\n",
    "9. IMPORTANT: If you get an accuracy near 0.500 in both training and validation set, try to reduce the learning rate to 0.00001 for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following lines in order to set up the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We set the \"seed\" so we make the results a bit more predictable.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type 'sample/' if you want to work on a smaller dataset.\n",
    "path = ''\n",
    "# Depending on your GPU you should change this. For a GTX 970 this is a good value. \n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the timestamp that we are going to use when saving files.\n",
    "timestamp = '102714012017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some useful paths to save files (e.g weights)\n",
    "files_path = path + 'files/'\n",
    "models_path = path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batches(path, shuffle=[False, False, True], augmentation=False):\n",
    "    \"\"\"\n",
    "    Load different batches that we'll use in our calculations.\n",
    "    \"\"\"\n",
    "\n",
    "    gen = image.ImageDataGenerator()\n",
    "    val_batches = gen.flow_from_directory(path + 'valid', target_size=(224,224),\n",
    "                    class_mode='categorical', shuffle=shuffle[0], batch_size=batch_size)\n",
    "    test_batches = gen.flow_from_directory(path + 'test', target_size=(224,224),\n",
    "                    class_mode='categorical', shuffle=shuffle[1], batch_size=batch_size)\n",
    "    \n",
    "    # We only want Data augmentation for the training set.\n",
    "    if augmentation:\n",
    "        gen = image.ImageDataGenerator(rotation_range=20, width_shift_range=0.1, shear_range=0.05,\n",
    "                                       height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "    train_batches = gen.flow_from_directory(path + 'train', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=shuffle[2], batch_size=batch_size)\n",
    "\n",
    "    return train_batches, val_batches, test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finetune(model):\n",
    "    \"\"\"\n",
    "    Removes the last layer (usually Dense) and replace it by another one more fitting.\n",
    "    This is useful when using a pre-trained model like VGG.\n",
    "    \"\"\"\n",
    "    model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.add(Dense(train_batches.nb_class, activation='softmax'))\n",
    "    model.compile(optimizer=RMSprop(lr=0.01, rho=0.7),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagation(model):\n",
    "    \"\"\"\n",
    "    Now we do Backpropagation. Backpropagation is when we want to train not only the last\n",
    "    Dense layer, but also some previous ones. Note that we don't train Convolutional layers.\n",
    "    \"\"\"\n",
    "    layers = model.layers\n",
    "    for layer in layers: layer.trainable=False\n",
    "    # Get the index of the first dense layer...\n",
    "    first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "    # ...and set this and all subsequent layers to trainable\n",
    "    for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(model, path, name, timestamp):\n",
    "    print 'Saving weights: {}.h5'.format(path + name + '_' + timestamp)\n",
    "    model.save_weights(path + '{}_{}.h5'.format(name, timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model, filepath):\n",
    "    print 'Loading weights: {}'.format(filepath)\n",
    "    model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_batches, val_batches, rules, name, timestamp):\n",
    "    \"\"\"\n",
    "    Rules will be something like:\n",
    "        (\n",
    "            (0.01, 3),\n",
    "            (0.1, 2),\n",
    "            ...\n",
    "        )\n",
    "    \"\"\"\n",
    "    for lr, epochs in rules:\n",
    "        model.compile(optimizer=RMSprop(lr=lr, rho=0.7),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print 'Lr: {}, Epoch: {}'.format(lr, i + 1)\n",
    "            model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, verbose=2,\n",
    "                               nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "            \n",
    "            #sys.stdout = open('keras_output.txt', 'w')\n",
    "            #history = model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, verbose=2,\n",
    "            #                              nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "            #sys.stdout = sys.__stdout__\n",
    "            #with open('keras_output.txt') as f:\n",
    "            #    content = f.readlines()\n",
    "            save_weights(model, files_path, '{}_lr{}_epoch{}'.format(\n",
    "                    name, lr, i+1), timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_conv_fc(model):\n",
    "    \"\"\"\n",
    "    Split Convolutional and Dense Layers.\n",
    "    \"\"\"\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_layers, fc_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the weights from the pre-trained model.\n",
    "# NB: Since we're removing dropout, we want to half the weights\n",
    "def proc_wgts(layer): return [o/2 for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model(conv_layers, fc_layers):\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.00001, rho=0.7), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version has the most basic possible configuration using the VGG16 pre-trained network. Please try to understand everything before moving forward.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Create simple model\n",
    "1. Load batches (train, valid and test)\n",
    "2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'default_parameter_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. Create simple model\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load batches (train, valid and test)\n",
    "train_batches, val_batches, test_batches = load_batches(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "finetune(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Train the model\n",
    "train_model(vgg.model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.01, 1),),\n",
    "            name + '_lastlayer',\n",
    "            timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_weights(vgg.model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Data Augmentation\" is a technic to reduce \"over-fitting\", where a generator slightly modifies the images we load \"on-the-fly\" so the model cannot adapt too much to our training data.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Create simple model\n",
    "1. Load batches (train, valid and test) with Data Augmentation (random changes to the images we load)\n",
    "2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'data_augmentation_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. Create simple model\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load batches (train, valid and test) with Data Augmentation (random changes to the images we load)\n",
    "train_batches, val_batches, test_batches = load_batches(path, augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "finetune(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Train the model\n",
    "train_model(vgg.model, train_batches, val_batches, ((0.01, 1), (0.1, 1), (0.001, 1), (0.0001, 1)), name + '_lastlayer', timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_weights(vgg.model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation - Only Dense Layers (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Backpropagation\" is the method of iteratively changing the weights of previous layers, not only the last one. When doing that for \"Convolutional layers\" we need to be very careful as it takes A LOT of memory.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Create simple model\n",
    "1. Load batches (train, valid and test)\n",
    "2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "3. Train first the last layer of the model. This way we are going to improve the overall accuracy.\n",
    "4. Set a \"trainable\" ONLY the dense layers.\n",
    "5. Train all the dense layers. Keep in mind that here the learning rate MUST be really small, as we assume that the pre-trained model is relatively good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'backpropagation_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. Create simple model\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load batches (train, valid and test)\n",
    "train_batches, val_batches, test_batches = load_batches(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "finetune(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "3. Train first the last layer of the model. This way we are going to improve the overall accuracy.\n",
    "train_model(vgg.model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.01, 1)),\n",
    "            name + '_lastlayer',\n",
    "            timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Set a \"trainable\" ALL the dense layers.\n",
    "backpropagation(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Train all the dense layers. Keep in mind that here the learning rate MUST be really small, as we assume that the pre-trained model is relatively good.\n",
    "train_model(vgg.model, train_batches, val_batches, ((0.0001, 1), (0.00001, 1)), name + '_denselayers', timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_weights(vgg.model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation + Backpropagation (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try the two methods together. Let's see if this improves the accuracy.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Create simple model\n",
    "1. Load batches (train, valid and test) with Data Augmentation (random changes to the images we load)\n",
    "2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "3. Train first the last layer of the model. This way we are going to improve the overall accuracy.\n",
    "4. Set a \"trainable\" ONLY the dense layers.\n",
    "5. Train all the dense layers. Keep in mind that here the learning rate MUST be really small, as we assume that the pre-trained model is relatively good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'data_augmentation_backpropagation_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. Create simple model\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Load batches (train, valid and test) with Data Augmentation (random changes to the images we load)\n",
    "train_batches, val_batches, test_batches = load_batches(path, augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Finetune the model (replace the last dense layer by one that has only two outputs in this case)\n",
    "finetune(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Train first the last layer of the model. This way we are going to improve the overall accuracy.\n",
    "train_model(vgg.model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.01, 6), (0.001, 3), (0.0001, 3)),\n",
    "            name + '_lastlayer', timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Set a \"trainable\" ONLY the dense layers.\n",
    "backpropagation(vgg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Train all the dense layers. Keep in mind that here the learning rate MUST be really small, as we assume that the pre-trained model is relatively good.\n",
    "train_model(vgg.model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.0001, 1), (0.00001, 1)),\n",
    "            name + '_denselayers',\n",
    "            timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_weights(vgg.model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Dropout (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dropout\" is a regularization method that randomly removes a certain percent of activations from the previous layer. It's commonly used for networks that are \"under-fitting\", meaning that we are still throwing away useful information.\n",
    "\n",
    "Why we calculate the \"features\" before? Because we don't want to train the convolutional layers (it takes too long). By using the output of the convolutional layers we are using a simple linear model which it's extremly fast.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Create model, finetune it and load good weights that we calculated before.\n",
    "    * And load batches (train, valid and test)\n",
    "1. Split the layers into two groups: Convolutional layers and Dense layers.\n",
    "2. Create a model with only the Convolutional layers.\n",
    "3. Calculate the predictions of our train and valid data using this new model.\n",
    "    * We'll have something like: [0, 0, 0.12, 0.45, 0,...]\n",
    "    * The shape of the resulting array will be: (nb_samples, 512, 14, 14). This is a list of filters, so if we have 2000 images, we'll have for each image: 512 filters, where each filter is an array of 14x14.\n",
    "    * This will be the input of the next linear model.\n",
    "4. Get \"real\" classes for the data using `train_batches.classes`. e.g 1 0 0 0 1 0 1 0 0 (each number is the class of an image)\n",
    "5. Transform those classes in OneHot format. e.g [0,0,1,0...] per each image\n",
    "6. Create a new linear model that has this array as an input.\n",
    "    * Because we removed the Dropout, now certain layers have the double number of inputs as before.\n",
    "    * To fix that, we remove the half of the weights on those layers, so we replicate the behavior of Dropout. e.g `for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))`\n",
    "7. We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'remove_dropout_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1) Create model\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1b) And load batches (train, valid and test)\n",
    "train_batches, val_batches, test_batches = load_batches(path, shuffle=[False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1c) finetune it!\n",
    "finetune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1d) Load good weights that we calculated before [This is an example, please change the path]\n",
    "load_weights(model, 'files/data_augmentation_backpropagation_vgg16_lastlayer_lr0.0001_epoch2_144813012017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# 2) Split the layers into two groups: Convolutional layers and Dense layers (or fully connected).\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                 if type(layer) is Convolution2D][-1]\n",
    "print last_conv_idx\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3) Create a model with only the Convolutional layers.\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4) Calculate the predictions.\n",
    "# The shape of the resulting array will be: (nb_samples, 512, 14, 14).\n",
    "# This is a list of filters, so if we have 2000 images, we'll have for\n",
    "# each image: 512 filters, where each filter is an array of 14x14.\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "train_features = conv_model.predict_generator(train_batches, train_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5) Get \"real\" classes for the data using train_batches.classes. e.g 1 0 0 0 1 0 1 0 0 (each number is the class of an image)\n",
    "val_classes = val_batches.classes\n",
    "train_classes = train_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6) Transform those classes in OneHot format. e.g [0,0,1,0...] per each image\n",
    "val_labels = onehot(val_classes)\n",
    "train_labels = onehot(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Save features\n",
    "save_array(models_path + 'debugging.bc'.format(timestamp), train_features)\n",
    "save_array(models_path + 'debugging.bc'.format(timestamp), val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: Load features\n",
    "train_features = load_array(models_path+'train_convlayer_features_144813012017_2.bc'.format(timestamp))\n",
    "val_features = load_array(models_path+'valid_convlayer_features_144813012017_2.bc'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional. Look at the shape of the input of the model that we are about to create:\n",
    "conv_layers[-1].output_shape[1:]\n",
    "# It should have the same shape as the last convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7) Create a new linear model that has this features as an input.\n",
    "fc_model = get_fc_model(conv_layers, fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional. Look at the model we've just created:\n",
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8) We train the model\n",
    "fc_model.fit(train_features, train_labels, nb_epoch=1, verbose=2,\n",
    "          batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: We save the weights\n",
    "save_weights(fc_model, files_path, name + '_9813', timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights: models/no_dropout.h5\n"
     ]
    }
   ],
   "source": [
    "# Optional: Load weights\n",
    "load_weights(fc_model, 'models/no_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = fc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data Augmentation to Dropout 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are over-fitting, let's add Data Augmentation to the previous method.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Load batches (train, valid and test)\n",
    "1. Get previous Fully connected model (linear model)\n",
    "2. Add this fully connected model to the convolutional model we created before\n",
    "3. Check that the new model is correct\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'data_augmentation_plus_dropout0_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1b) And load batches (train, valid and test)\n",
    "train_batches, val_batches, test_batches = load_batches(path, augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Get previous Fully connected model (linear model)\n",
    "# CAREFUL! This will replace the existing weights! Leave it commented out if you want to re-use the weights\n",
    "conv_model = Sequential(conv_layers)\n",
    "fc_model = get_fc_model(conv_layers, fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. Add this fully connected model to the convolutional model we created before\n",
    "# We need to do this because we don't want to train the convolutional layers.\n",
    "#conv_model = Sequential(conv_layers)\n",
    "for layer in conv_model.layers: layer.trainable = False\n",
    "# Look how easy it is to connect two models together!\n",
    "conv_model.add(fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "                                                                   lambda_1[0][0]                   \n",
      "                                                                   lambda_1[0][0]                   \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "                                                                   zeropadding2d_1[1][0]            \n",
      "                                                                   zeropadding2d_1[2][0]            \n",
      "                                                                   zeropadding2d_1[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "                                                                   convolution2d_1[1][0]            \n",
      "                                                                   convolution2d_1[2][0]            \n",
      "                                                                   convolution2d_1[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "                                                                   zeropadding2d_2[1][0]            \n",
      "                                                                   zeropadding2d_2[2][0]            \n",
      "                                                                   zeropadding2d_2[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "                                                                   convolution2d_2[1][0]            \n",
      "                                                                   convolution2d_2[2][0]            \n",
      "                                                                   convolution2d_2[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "                                                                   maxpooling2d_1[1][0]             \n",
      "                                                                   maxpooling2d_1[2][0]             \n",
      "                                                                   maxpooling2d_1[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "                                                                   zeropadding2d_3[1][0]            \n",
      "                                                                   zeropadding2d_3[2][0]            \n",
      "                                                                   zeropadding2d_3[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "                                                                   convolution2d_3[1][0]            \n",
      "                                                                   convolution2d_3[2][0]            \n",
      "                                                                   convolution2d_3[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "                                                                   zeropadding2d_4[1][0]            \n",
      "                                                                   zeropadding2d_4[2][0]            \n",
      "                                                                   zeropadding2d_4[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "                                                                   convolution2d_4[1][0]            \n",
      "                                                                   convolution2d_4[2][0]            \n",
      "                                                                   convolution2d_4[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "                                                                   maxpooling2d_2[1][0]             \n",
      "                                                                   maxpooling2d_2[2][0]             \n",
      "                                                                   maxpooling2d_2[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "                                                                   zeropadding2d_5[1][0]            \n",
      "                                                                   zeropadding2d_5[2][0]            \n",
      "                                                                   zeropadding2d_5[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "                                                                   convolution2d_5[1][0]            \n",
      "                                                                   convolution2d_5[2][0]            \n",
      "                                                                   convolution2d_5[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "                                                                   zeropadding2d_6[1][0]            \n",
      "                                                                   zeropadding2d_6[2][0]            \n",
      "                                                                   zeropadding2d_6[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "                                                                   convolution2d_6[1][0]            \n",
      "                                                                   convolution2d_6[2][0]            \n",
      "                                                                   convolution2d_6[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "                                                                   zeropadding2d_7[1][0]            \n",
      "                                                                   zeropadding2d_7[2][0]            \n",
      "                                                                   zeropadding2d_7[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "                                                                   convolution2d_7[1][0]            \n",
      "                                                                   convolution2d_7[2][0]            \n",
      "                                                                   convolution2d_7[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "                                                                   maxpooling2d_3[1][0]             \n",
      "                                                                   maxpooling2d_3[2][0]             \n",
      "                                                                   maxpooling2d_3[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "                                                                   zeropadding2d_8[1][0]            \n",
      "                                                                   zeropadding2d_8[2][0]            \n",
      "                                                                   zeropadding2d_8[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "                                                                   convolution2d_8[1][0]            \n",
      "                                                                   convolution2d_8[2][0]            \n",
      "                                                                   convolution2d_8[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "                                                                   zeropadding2d_9[1][0]            \n",
      "                                                                   zeropadding2d_9[2][0]            \n",
      "                                                                   zeropadding2d_9[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "                                                                   convolution2d_9[1][0]            \n",
      "                                                                   convolution2d_9[2][0]            \n",
      "                                                                   convolution2d_9[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "                                                                   zeropadding2d_10[1][0]           \n",
      "                                                                   zeropadding2d_10[2][0]           \n",
      "                                                                   zeropadding2d_10[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "                                                                   convolution2d_10[1][0]           \n",
      "                                                                   convolution2d_10[2][0]           \n",
      "                                                                   convolution2d_10[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "                                                                   maxpooling2d_4[1][0]             \n",
      "                                                                   maxpooling2d_4[2][0]             \n",
      "                                                                   maxpooling2d_4[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "                                                                   zeropadding2d_11[1][0]           \n",
      "                                                                   zeropadding2d_11[2][0]           \n",
      "                                                                   zeropadding2d_11[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "                                                                   convolution2d_11[1][0]           \n",
      "                                                                   convolution2d_11[2][0]           \n",
      "                                                                   convolution2d_11[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "                                                                   zeropadding2d_12[1][0]           \n",
      "                                                                   zeropadding2d_12[2][0]           \n",
      "                                                                   zeropadding2d_12[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "                                                                   convolution2d_12[1][0]           \n",
      "                                                                   convolution2d_12[2][0]           \n",
      "                                                                   convolution2d_12[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "                                                                   zeropadding2d_13[1][0]           \n",
      "                                                                   zeropadding2d_13[2][0]           \n",
      "                                                                   zeropadding2d_13[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)        (None, 2)             119554050   convolution2d_13[3][0]           \n",
      "====================================================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 14,714,688\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Check that the new model is correct\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Train the model\n",
    "train_model(conv_model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.000001, 2),),\n",
    "            name + '_data_augentation_to_zero_dropout',\n",
    "            timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: We save the weights\n",
    "save_weights(conv_model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization (batchnorm) is a way to ensure that activations don't become too high or too low at any point in the model. Adjusting activations so they are of similar scales is called normalization. It's a MUST TO HAVE as you can improve the training speed up to 10x.\n",
    "\n",
    "### What do we want to do?\n",
    "\n",
    "0. Check the current shape of the convolutional layers\n",
    "1. Create model with Batch Normalization\n",
    "2. Finetune it and adjust weights based on the new Dropout number\n",
    "3. Train the Batch Normalization model\n",
    "4. Create a final model based on the \"convolutional layers\"\n",
    "5. Set as \"non-trainable\" to all the layers of this last model\n",
    "6. Add all the new layers from the Batch Normalization model to this last model.\n",
    "7. Set weights of the new added layers. Apparently, when you add a layer to a model, the weights are not copied through, so this step is required!\n",
    "8. Train the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'batch_normalization_vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Check the current shape of the convolutional layers\n",
    "conv_layers[-1].output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(1000, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Create model with Batch Normalization\n",
    "p = 0.6\n",
    "bn_model = Sequential(get_bn_layers(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_wgts_bn(layer, prev_p, new_p):\n",
    "    scal = (1-prev_p)/(1-new_p)\n",
    "    return [o*scal for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Finetune it and adjust weights based on the new Dropout number\n",
    "for l in bn_model.layers: \n",
    "    if type(l)==Dense: l.set_weights(proc_wgts_bn(l, 0.3, 0.6))\n",
    "\n",
    "finetune(bn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Train the Batch Normalization model\n",
    "bn_model.fit(train_features, train_labels, nb_epoch=1, verbose=2,\n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: We save the weights\n",
    "save_weights(bn_model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Create a final model based on the \"convolutional layers\"\n",
    "bn_layers = get_bn_layers(0.6)\n",
    "bn_layers.pop()\n",
    "bn_layers.append(Dense(2,activation='softmax'))\n",
    "\n",
    "final_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Set as \"non-trainable\" to all the layers of this last model\n",
    "for layer in final_model.layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7. Add all the new layers from the Batch Normalization model to this last model.\n",
    "for layer in bn_layers: final_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8. Set weights of the new added layers. Apparently, when you add a layer to a model, the weights are not copied through, so this step is required!\n",
    "for l1,l2 in zip(bn_model.layers, bn_layers):\n",
    "    l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 9. Train the last model\n",
    "train_model(final_model,\n",
    "            train_batches,\n",
    "            val_batches,\n",
    "            ((0.000001, 1),),\n",
    "            name + '_batch_normalization',\n",
    "            timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional: We save the weights\n",
    "save_weights(bn_model, files_path, name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing model prediction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A few correct labels at random\n",
    "* A few incorrect labels at random\n",
    "* The most correct labels of each class (ie those with highest probability that are correct)\n",
    "* The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "* The most uncertain labels (ie those with probability closest to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(path + 'valid', batch_size = batch_size)\n",
    "\n",
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1\n",
    "\n",
    "#Round our predictions to 0/1 to generate labels\n",
    "our_predictions = probs[:,0]\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#Helper function to plot images by index in the validation set \n",
    "#Plots is a helper function in utils.py\n",
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(path + 'valid/' + filenames[i]) for i in idx], titles=titles)\n",
    "    \n",
    "#Number of images to view for each visualization task\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. A few correct labels at random\n",
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print \"Found %d correct labels\" % len(correct)\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. A few incorrect labels at random\n",
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print \"Found %d incorrect labels\" % len(incorrect)\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3a. The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print \"Found %d confident correct cats labels\" % len(correct_cats)\n",
    "most_correct_cats = np.argsort(our_predictions[correct_cats])[::-1][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], our_predictions[correct_cats][most_correct_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3b. The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print \"Found %d confident correct dogs labels\" % len(correct_dogs)\n",
    "most_correct_dogs = np.argsort(our_predictions[correct_dogs])[:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], our_predictions[correct_dogs][most_correct_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4a. The images we were most confident were cats, but are actually dogs\n",
    "incorrect_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print \"Found %d incorrect cats\" % len(incorrect_cats)\n",
    "if len(incorrect_cats):\n",
    "    most_incorrect_cats = np.argsort(our_predictions[incorrect_cats])[::-1][:n_view]\n",
    "    plots_idx(incorrect_cats[most_incorrect_cats], our_predictions[incorrect_cats][most_incorrect_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4b. The images we were most confident were dogs, but are actually cats\n",
    "incorrect_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print \"Found %d incorrect dogs\" % len(incorrect_dogs)\n",
    "if len(incorrect_dogs):\n",
    "    most_incorrect_dogs = np.argsort(our_predictions[incorrect_dogs])[:n_view]\n",
    "    plots_idx(incorrect_dogs[most_incorrect_dogs], our_predictions[incorrect_dogs][most_incorrect_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "most_uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "plots_idx(most_uncertain[:n_view], our_predictions[most_uncertain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dim_ordering='tf' uses tensorflow dimension ordering,\n",
    "#   which is the same order as matplotlib uses for display.\n",
    "# Therefore when just using for display purposes, this is more convenient\n",
    "gen = image.ImageDataGenerator(rotation_range=20, width_shift_range=0.1, shear_range=0.05,\n",
    "                               height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True,dim_ordering='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a 'batch' of a single image\n",
    "img = np.expand_dims(ndimage.imread(path+'test/unknown/87.jpg'),0)\n",
    "# Request the generator to create batches from this image\n",
    "aug_iter = gen.flow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get eight examples of these augmented images\n",
    "aug_imgs = [next(aug_iter)[0].astype(np.uint8) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The original\n",
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Augmented data\n",
    "plots(aug_imgs, (20,7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure that we return to theano dimension ordering\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confussion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test set + create Kaggle submission file (taught in the Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = fc_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isdog = predictions[:,1]\n",
    "print \"Raw Predictions: \" + str(isdog[:5])\n",
    "print \"Mid Predictions: \" + str(isdog[(isdog < .6) & (isdog > .4)])\n",
    "print \"Edge Predictions: \" + str(isdog[(isdog == 1) | (isdog == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isdog = isdog.clip(min=0.05, max=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = test_batches.filenames\n",
    "\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_file_name = 'submission_{}_5.csv'.format(timestamp)\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(submission_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative way to generate Submission file (it has a better score!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights: files/data_augmentation_plus_dropout0_vgg16_data_augentation_to_zero_dropout_lr1e-05_epoch1_102714012017.h5\n"
     ]
    }
   ],
   "source": [
    "load_weights(conv_model, 'files/data_augmentation_plus_dropout0_vgg16_data_augentation_to_zero_dropout_lr1e-05_epoch1_102714012017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_csv(submission_file_name, data, columns):\n",
    "    \"\"\"\n",
    "    Write data according to the Kaggle submission format.\n",
    "    \"\"\"\n",
    "    with open(submission_file_name, 'wb') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(columns)\n",
    "        for key in data.keys():\n",
    "            w.writerow([key, data[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "test_batches = gen.flow_from_directory(path + 'test', target_size=(224,224),\n",
    "                                       class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]\n",
    "#conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "d = {}\n",
    "submission_file_name = 'submission_{}_5_new.csv'.format(timestamp)\n",
    "for idx, filename in enumerate(test_batches.filenames):\n",
    "    # We only want the ID, so remove the folder name and file extension.\n",
    "    result = int(filename[8:-4])\n",
    "    # We use a trick to never show 0 or 1, but 0.05 and 0.95.\n",
    "    # This is required becase log loss penalizes predictions that are confident and wrong.\n",
    "    d[result] = predictions[idx][1].clip(min=0.05, max=0.95)\n",
    "write_submission_csv(submission_file_name, d, ['id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(submission_file_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
